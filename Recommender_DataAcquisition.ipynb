{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 URLs\n",
    "\n",
    "Basenotes.net has 5,315 pages of fragrance reviews. To scrape review data from these pages I first need a list of URLs, one for each page of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL without the page numbers\n",
    "base_url = 'http://www.basenotes.net/fragrancereviews/page/'\n",
    "\n",
    "# range of page numbers\n",
    "page_range = range(1, 5316)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_list(base_url, id_list):\n",
    "    '''Create a list of URLs to be scraped.'''\n",
    "    url_list = []\n",
    "    \n",
    "    for id in id_list:\n",
    "        url = base_url + str(id)\n",
    "        url_list.append(url)\n",
    "        \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through page numbers and create a list of URLs\n",
    "url_list = create_url_list(base_url, page_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5315"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for correct number of URLs\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls_to_soups(url_list):\n",
    "    '''Parse soup from each URL and append to a list.'''\n",
    "    soup_list = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        response = requests.get(url)\n",
    "    \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            soup_list.append(soup)\n",
    "            sleep(7)\n",
    "        else:\n",
    "            print(response.status_code)\n",
    "    \n",
    "    return soup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n",
      "503\n",
      "503\n",
      "503\n"
     ]
    }
   ],
   "source": [
    "soup_list = urls_to_soups(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5311"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soups_to_dataframe(soup_list):\n",
    "    '''Parse data from soup for each review page and return a dataframe.'''\n",
    "    for soup in soup_list:\n",
    "        # if the first page of reviews, parse data, and add to a new dataframe\n",
    "        if soup_list.index(soup) == 0:\n",
    "            \n",
    "            # parse reviews from first page\n",
    "            reviews = soup.find_all('div', {'class': 'reviewmain'})\n",
    "    \n",
    "            reviews_list = []\n",
    "    \n",
    "            for review in reviews:\n",
    "                # create a dictionary to store data from each review\n",
    "                review_data_dict = {}\n",
    "\n",
    "                # parse data from review class\n",
    "                rating_value = review['class'][-1][-1]\n",
    "                review_id = review['id'].split('-')[-1]\n",
    "\n",
    "                # parse reivewauthor class\n",
    "                user_section = review.find('div', {'class': 'reviewauthor'})\n",
    "\n",
    "                # parse data from user_section\n",
    "                user_id = user_section.find('a')['href'].split('/')[-1]\n",
    "                user_name = user_section.text.split(' Show all reviews ')[0]\n",
    "                user_location = user_section.text.split(' Show all reviews ')[-1]\n",
    "\n",
    "                # parse reviewblurb class\n",
    "                blurb_section = review.find('div', {'class': 'reviewblurb'})\n",
    "\n",
    "                # parse data from blurb_section\n",
    "                scent_id = blurb_section.find('a')['href'].split('ID')[-1].split('.')[0]\n",
    "                scent_name = blurb_section.find('a').text.split(' by ')[0]\n",
    "                scent_brand = blurb_section.find('a').text.split(' by ')[-1]\n",
    "                review_text = blurb_section.text.split(scent_brand, 1)[-1]\n",
    "\n",
    "                # add data for each field to the dictionary\n",
    "                review_data_dict['rating_value'] = rating_value\n",
    "                review_data_dict['review_id'] = review_id\n",
    "                review_data_dict['user_id'] = user_id\n",
    "                review_data_dict['user_name'] = user_name\n",
    "                review_data_dict['user_location'] = user_location\n",
    "                review_data_dict['scent_id'] = scent_id\n",
    "                review_data_dict['scent_name'] = scent_name\n",
    "                review_data_dict['scent_brand'] = scent_brand\n",
    "                review_data_dict['review_text'] = review_text\n",
    "\n",
    "                #append dictionary to list\n",
    "                reviews_list.append(review_data_dict)\n",
    "        \n",
    "            \n",
    "            # create a dataframe from the list of dictionaries\n",
    "            reviews_df = pd.DataFrame(reviews_list)\n",
    "            \n",
    "        # for each additional page, parse the data and append to the dataframe\n",
    "        else:\n",
    "                \n",
    "            # parse reviews from first page\n",
    "            reviews = soup.find_all('div', {'class': 'reviewmain'})\n",
    "    \n",
    "            reviews_list = []\n",
    "    \n",
    "            for review in reviews:\n",
    "                # create a dictionary to store data from each review\n",
    "                review_data_dict = {}\n",
    "\n",
    "                # parse data from review class\n",
    "                rating_value = review['class'][-1][-1]\n",
    "                review_id = review['id'].split('-')[-1]\n",
    "\n",
    "                # parse reivewauthor class\n",
    "                user_section = review.find('div', {'class': 'reviewauthor'})\n",
    "\n",
    "                # parse data from user_section\n",
    "                user_id = user_section.find('a')['href'].split('/')[-1]\n",
    "                user_name = user_section.text.split(' Show all reviews ')[0]\n",
    "                user_location = user_section.text.split(' Show all reviews ')[-1]\n",
    "\n",
    "                # parse reviewblurb class\n",
    "                blurb_section = review.find('div', {'class': 'reviewblurb'})\n",
    "\n",
    "                # parse data from blurb_section\n",
    "                scent_id = blurb_section.find('a')['href'].split('ID')[-1].split('.')[0]\n",
    "                scent_name = blurb_section.find('a').text.split(' by ')[0]\n",
    "                scent_brand = blurb_section.find('a').text.split(' by ')[-1]\n",
    "                review_text = blurb_section.text.split(scent_brand, 1)[-1]\n",
    "\n",
    "                # add data for each field to the dictionary\n",
    "                review_data_dict['rating_value'] = rating_value\n",
    "                review_data_dict['review_id'] = review_id\n",
    "                review_data_dict['user_id'] = user_id\n",
    "                review_data_dict['user_name'] = user_name\n",
    "                review_data_dict['user_location'] = user_location\n",
    "                review_data_dict['scent_id'] = scent_id\n",
    "                review_data_dict['scent_name'] = scent_name\n",
    "                review_data_dict['scent_brand'] = scent_brand\n",
    "                review_data_dict['review_text'] = review_text\n",
    "                \n",
    "                #append dictionary to list\n",
    "                reviews_list.append(review_data_dict)\n",
    "        \n",
    "            # append data to dataframe\n",
    "            reviews_df = reviews_df.append(reviews_list, ignore_index=True)\n",
    "        \n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = soups_to_dataframe(soup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to CSV\n",
    "reviews_df.to_csv('basenotes_reviews_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
